{"ast":null,"code":"// src/openai-compatible-chat-language-model.ts\nimport { InvalidResponseDataError } from \"@ai-sdk/provider\";\nimport { combineHeaders, createEventSourceResponseHandler, createJsonErrorResponseHandler, createJsonResponseHandler, generateId, isParsableJson, postJsonToApi } from \"@ai-sdk/provider-utils\";\nimport { z as z2 } from \"zod\";\n\n// src/convert-to-openai-compatible-chat-messages.ts\nimport { UnsupportedFunctionalityError } from \"@ai-sdk/provider\";\nimport { convertUint8ArrayToBase64 } from \"@ai-sdk/provider-utils\";\nfunction getOpenAIMetadata(message) {\n  var _a, _b;\n  return (_b = (_a = message == null ? void 0 : message.providerMetadata) == null ? void 0 : _a.openaiCompatible) != null ? _b : {};\n}\nfunction convertToOpenAICompatibleChatMessages(prompt) {\n  const messages = [];\n  for (const {\n    role,\n    content,\n    ...message\n  } of prompt) {\n    const metadata = getOpenAIMetadata({\n      ...message\n    });\n    switch (role) {\n      case \"system\":\n        {\n          messages.push({\n            role: \"system\",\n            content,\n            ...metadata\n          });\n          break;\n        }\n      case \"user\":\n        {\n          if (content.length === 1 && content[0].type === \"text\") {\n            messages.push({\n              role: \"user\",\n              content: content[0].text,\n              ...getOpenAIMetadata(content[0])\n            });\n            break;\n          }\n          messages.push({\n            role: \"user\",\n            content: content.map(part => {\n              var _a;\n              const partMetadata = getOpenAIMetadata(part);\n              switch (part.type) {\n                case \"text\":\n                  {\n                    return {\n                      type: \"text\",\n                      text: part.text,\n                      ...partMetadata\n                    };\n                  }\n                case \"image\":\n                  {\n                    return {\n                      type: \"image_url\",\n                      image_url: {\n                        url: part.image instanceof URL ? part.image.toString() : `data:${(_a = part.mimeType) != null ? _a : \"image/jpeg\"};base64,${convertUint8ArrayToBase64(part.image)}`\n                      },\n                      ...partMetadata\n                    };\n                  }\n                case \"file\":\n                  {\n                    throw new UnsupportedFunctionalityError({\n                      functionality: \"File content parts in user messages\"\n                    });\n                  }\n              }\n            }),\n            ...metadata\n          });\n          break;\n        }\n      case \"assistant\":\n        {\n          let text = \"\";\n          const toolCalls = [];\n          for (const part of content) {\n            const partMetadata = getOpenAIMetadata(part);\n            switch (part.type) {\n              case \"text\":\n                {\n                  text += part.text;\n                  break;\n                }\n              case \"tool-call\":\n                {\n                  toolCalls.push({\n                    id: part.toolCallId,\n                    type: \"function\",\n                    function: {\n                      name: part.toolName,\n                      arguments: JSON.stringify(part.args)\n                    },\n                    ...partMetadata\n                  });\n                  break;\n                }\n            }\n          }\n          messages.push({\n            role: \"assistant\",\n            content: text,\n            tool_calls: toolCalls.length > 0 ? toolCalls : void 0,\n            ...metadata\n          });\n          break;\n        }\n      case \"tool\":\n        {\n          for (const toolResponse of content) {\n            const toolResponseMetadata = getOpenAIMetadata(toolResponse);\n            messages.push({\n              role: \"tool\",\n              tool_call_id: toolResponse.toolCallId,\n              content: JSON.stringify(toolResponse.result),\n              ...toolResponseMetadata\n            });\n          }\n          break;\n        }\n      default:\n        {\n          const _exhaustiveCheck = role;\n          throw new Error(`Unsupported role: ${_exhaustiveCheck}`);\n        }\n    }\n  }\n  return messages;\n}\n\n// src/get-response-metadata.ts\nfunction getResponseMetadata({\n  id,\n  model,\n  created\n}) {\n  return {\n    id: id != null ? id : void 0,\n    modelId: model != null ? model : void 0,\n    timestamp: created != null ? new Date(created * 1e3) : void 0\n  };\n}\n\n// src/map-openai-compatible-finish-reason.ts\nfunction mapOpenAICompatibleFinishReason(finishReason) {\n  switch (finishReason) {\n    case \"stop\":\n      return \"stop\";\n    case \"length\":\n      return \"length\";\n    case \"content_filter\":\n      return \"content-filter\";\n    case \"function_call\":\n    case \"tool_calls\":\n      return \"tool-calls\";\n    default:\n      return \"unknown\";\n  }\n}\n\n// src/openai-compatible-error.ts\nimport { z } from \"zod\";\nvar openaiCompatibleErrorDataSchema = z.object({\n  error: z.object({\n    message: z.string(),\n    // The additional information below is handled loosely to support\n    // OpenAI-compatible providers that have slightly different error\n    // responses:\n    type: z.string().nullish(),\n    param: z.any().nullish(),\n    code: z.union([z.string(), z.number()]).nullish()\n  })\n});\nvar defaultOpenAICompatibleErrorStructure = {\n  errorSchema: openaiCompatibleErrorDataSchema,\n  errorToMessage: data => data.error.message\n};\n\n// src/openai-compatible-prepare-tools.ts\nimport { UnsupportedFunctionalityError as UnsupportedFunctionalityError2 } from \"@ai-sdk/provider\";\nfunction prepareTools({\n  mode,\n  structuredOutputs\n}) {\n  var _a;\n  const tools = ((_a = mode.tools) == null ? void 0 : _a.length) ? mode.tools : void 0;\n  const toolWarnings = [];\n  if (tools == null) {\n    return {\n      tools: void 0,\n      tool_choice: void 0,\n      toolWarnings\n    };\n  }\n  const toolChoice = mode.toolChoice;\n  const openaiCompatTools = [];\n  for (const tool of tools) {\n    if (tool.type === \"provider-defined\") {\n      toolWarnings.push({\n        type: \"unsupported-tool\",\n        tool\n      });\n    } else {\n      openaiCompatTools.push({\n        type: \"function\",\n        function: {\n          name: tool.name,\n          description: tool.description,\n          parameters: tool.parameters\n        }\n      });\n    }\n  }\n  if (toolChoice == null) {\n    return {\n      tools: openaiCompatTools,\n      tool_choice: void 0,\n      toolWarnings\n    };\n  }\n  const type = toolChoice.type;\n  switch (type) {\n    case \"auto\":\n    case \"none\":\n    case \"required\":\n      return {\n        tools: openaiCompatTools,\n        tool_choice: type,\n        toolWarnings\n      };\n    case \"tool\":\n      return {\n        tools: openaiCompatTools,\n        tool_choice: {\n          type: \"function\",\n          function: {\n            name: toolChoice.toolName\n          }\n        },\n        toolWarnings\n      };\n    default:\n      {\n        const _exhaustiveCheck = type;\n        throw new UnsupportedFunctionalityError2({\n          functionality: `Unsupported tool choice type: ${_exhaustiveCheck}`\n        });\n      }\n  }\n}\n\n// src/openai-compatible-chat-language-model.ts\nvar OpenAICompatibleChatLanguageModel = class {\n  // type inferred via constructor\n  constructor(modelId, settings, config) {\n    this.specificationVersion = \"v1\";\n    var _a, _b;\n    this.modelId = modelId;\n    this.settings = settings;\n    this.config = config;\n    const errorStructure = (_a = config.errorStructure) != null ? _a : defaultOpenAICompatibleErrorStructure;\n    this.chunkSchema = createOpenAICompatibleChatChunkSchema(errorStructure.errorSchema);\n    this.failedResponseHandler = createJsonErrorResponseHandler(errorStructure);\n    this.supportsStructuredOutputs = (_b = config.supportsStructuredOutputs) != null ? _b : false;\n  }\n  get defaultObjectGenerationMode() {\n    return this.config.defaultObjectGenerationMode;\n  }\n  get provider() {\n    return this.config.provider;\n  }\n  get providerOptionsName() {\n    return this.config.provider.split(\".\")[0].trim();\n  }\n  getArgs({\n    mode,\n    prompt,\n    maxTokens,\n    temperature,\n    topP,\n    topK,\n    frequencyPenalty,\n    presencePenalty,\n    providerMetadata,\n    stopSequences,\n    responseFormat,\n    seed\n  }) {\n    var _a, _b, _c, _d, _e;\n    const type = mode.type;\n    const warnings = [];\n    if (topK != null) {\n      warnings.push({\n        type: \"unsupported-setting\",\n        setting: \"topK\"\n      });\n    }\n    if ((responseFormat == null ? void 0 : responseFormat.type) === \"json\" && responseFormat.schema != null && !this.supportsStructuredOutputs) {\n      warnings.push({\n        type: \"unsupported-setting\",\n        setting: \"responseFormat\",\n        details: \"JSON response format schema is only supported with structuredOutputs\"\n      });\n    }\n    const baseArgs = {\n      // model id:\n      model: this.modelId,\n      // model specific settings:\n      user: this.settings.user,\n      // standardized settings:\n      max_tokens: maxTokens,\n      temperature,\n      top_p: topP,\n      frequency_penalty: frequencyPenalty,\n      presence_penalty: presencePenalty,\n      response_format: (responseFormat == null ? void 0 : responseFormat.type) === \"json\" ? this.supportsStructuredOutputs === true && responseFormat.schema != null ? {\n        type: \"json_schema\",\n        json_schema: {\n          schema: responseFormat.schema,\n          name: (_a = responseFormat.name) != null ? _a : \"response\",\n          description: responseFormat.description\n        }\n      } : {\n        type: \"json_object\"\n      } : void 0,\n      stop: stopSequences,\n      seed,\n      ...(providerMetadata == null ? void 0 : providerMetadata[this.providerOptionsName]),\n      reasoning_effort: (_d = (_b = providerMetadata == null ? void 0 : providerMetadata[this.providerOptionsName]) == null ? void 0 : _b.reasoningEffort) != null ? _d : (_c = providerMetadata == null ? void 0 : providerMetadata[\"openai-compatible\"]) == null ? void 0 : _c.reasoningEffort,\n      // messages:\n      messages: convertToOpenAICompatibleChatMessages(prompt)\n    };\n    switch (type) {\n      case \"regular\":\n        {\n          const {\n            tools,\n            tool_choice,\n            toolWarnings\n          } = prepareTools({\n            mode,\n            structuredOutputs: this.supportsStructuredOutputs\n          });\n          return {\n            args: {\n              ...baseArgs,\n              tools,\n              tool_choice\n            },\n            warnings: [...warnings, ...toolWarnings]\n          };\n        }\n      case \"object-json\":\n        {\n          return {\n            args: {\n              ...baseArgs,\n              response_format: this.supportsStructuredOutputs === true && mode.schema != null ? {\n                type: \"json_schema\",\n                json_schema: {\n                  schema: mode.schema,\n                  name: (_e = mode.name) != null ? _e : \"response\",\n                  description: mode.description\n                }\n              } : {\n                type: \"json_object\"\n              }\n            },\n            warnings\n          };\n        }\n      case \"object-tool\":\n        {\n          return {\n            args: {\n              ...baseArgs,\n              tool_choice: {\n                type: \"function\",\n                function: {\n                  name: mode.tool.name\n                }\n              },\n              tools: [{\n                type: \"function\",\n                function: {\n                  name: mode.tool.name,\n                  description: mode.tool.description,\n                  parameters: mode.tool.parameters\n                }\n              }]\n            },\n            warnings\n          };\n        }\n      default:\n        {\n          const _exhaustiveCheck = type;\n          throw new Error(`Unsupported type: ${_exhaustiveCheck}`);\n        }\n    }\n  }\n  async doGenerate(options) {\n    var _a, _b, _c, _d, _e, _f, _g, _h, _i, _j, _k;\n    const {\n      args,\n      warnings\n    } = this.getArgs({\n      ...options\n    });\n    const body = JSON.stringify(args);\n    const {\n      responseHeaders,\n      value: responseBody,\n      rawValue: rawResponse\n    } = await postJsonToApi({\n      url: this.config.url({\n        path: \"/chat/completions\",\n        modelId: this.modelId\n      }),\n      headers: combineHeaders(this.config.headers(), options.headers),\n      body: args,\n      failedResponseHandler: this.failedResponseHandler,\n      successfulResponseHandler: createJsonResponseHandler(OpenAICompatibleChatResponseSchema),\n      abortSignal: options.abortSignal,\n      fetch: this.config.fetch\n    });\n    const {\n      messages: rawPrompt,\n      ...rawSettings\n    } = args;\n    const choice = responseBody.choices[0];\n    const providerMetadata = {\n      [this.providerOptionsName]: {},\n      ...((_b = (_a = this.config.metadataExtractor) == null ? void 0 : _a.extractMetadata) == null ? void 0 : _b.call(_a, {\n        parsedBody: rawResponse\n      }))\n    };\n    const completionTokenDetails = (_c = responseBody.usage) == null ? void 0 : _c.completion_tokens_details;\n    const promptTokenDetails = (_d = responseBody.usage) == null ? void 0 : _d.prompt_tokens_details;\n    if ((completionTokenDetails == null ? void 0 : completionTokenDetails.reasoning_tokens) != null) {\n      providerMetadata[this.providerOptionsName].reasoningTokens = completionTokenDetails == null ? void 0 : completionTokenDetails.reasoning_tokens;\n    }\n    if ((completionTokenDetails == null ? void 0 : completionTokenDetails.accepted_prediction_tokens) != null) {\n      providerMetadata[this.providerOptionsName].acceptedPredictionTokens = completionTokenDetails == null ? void 0 : completionTokenDetails.accepted_prediction_tokens;\n    }\n    if ((completionTokenDetails == null ? void 0 : completionTokenDetails.rejected_prediction_tokens) != null) {\n      providerMetadata[this.providerOptionsName].rejectedPredictionTokens = completionTokenDetails == null ? void 0 : completionTokenDetails.rejected_prediction_tokens;\n    }\n    if ((promptTokenDetails == null ? void 0 : promptTokenDetails.cached_tokens) != null) {\n      providerMetadata[this.providerOptionsName].cachedPromptTokens = promptTokenDetails == null ? void 0 : promptTokenDetails.cached_tokens;\n    }\n    return {\n      text: (_e = choice.message.content) != null ? _e : void 0,\n      reasoning: (_f = choice.message.reasoning_content) != null ? _f : void 0,\n      toolCalls: (_g = choice.message.tool_calls) == null ? void 0 : _g.map(toolCall => {\n        var _a2;\n        return {\n          toolCallType: \"function\",\n          toolCallId: (_a2 = toolCall.id) != null ? _a2 : generateId(),\n          toolName: toolCall.function.name,\n          args: toolCall.function.arguments\n        };\n      }),\n      finishReason: mapOpenAICompatibleFinishReason(choice.finish_reason),\n      usage: {\n        promptTokens: (_i = (_h = responseBody.usage) == null ? void 0 : _h.prompt_tokens) != null ? _i : NaN,\n        completionTokens: (_k = (_j = responseBody.usage) == null ? void 0 : _j.completion_tokens) != null ? _k : NaN\n      },\n      providerMetadata,\n      rawCall: {\n        rawPrompt,\n        rawSettings\n      },\n      rawResponse: {\n        headers: responseHeaders,\n        body: rawResponse\n      },\n      response: getResponseMetadata(responseBody),\n      warnings,\n      request: {\n        body\n      }\n    };\n  }\n  async doStream(options) {\n    var _a;\n    if (this.settings.simulateStreaming) {\n      const result = await this.doGenerate(options);\n      const simulatedStream = new ReadableStream({\n        start(controller) {\n          controller.enqueue({\n            type: \"response-metadata\",\n            ...result.response\n          });\n          if (result.reasoning) {\n            if (Array.isArray(result.reasoning)) {\n              for (const part of result.reasoning) {\n                if (part.type === \"text\") {\n                  controller.enqueue({\n                    type: \"reasoning\",\n                    textDelta: part.text\n                  });\n                }\n              }\n            } else {\n              controller.enqueue({\n                type: \"reasoning\",\n                textDelta: result.reasoning\n              });\n            }\n          }\n          if (result.text) {\n            controller.enqueue({\n              type: \"text-delta\",\n              textDelta: result.text\n            });\n          }\n          if (result.toolCalls) {\n            for (const toolCall of result.toolCalls) {\n              controller.enqueue({\n                type: \"tool-call\",\n                ...toolCall\n              });\n            }\n          }\n          controller.enqueue({\n            type: \"finish\",\n            finishReason: result.finishReason,\n            usage: result.usage,\n            logprobs: result.logprobs,\n            providerMetadata: result.providerMetadata\n          });\n          controller.close();\n        }\n      });\n      return {\n        stream: simulatedStream,\n        rawCall: result.rawCall,\n        rawResponse: result.rawResponse,\n        warnings: result.warnings\n      };\n    }\n    const {\n      args,\n      warnings\n    } = this.getArgs({\n      ...options\n    });\n    const body = {\n      ...args,\n      stream: true,\n      // only include stream_options when in strict compatibility mode:\n      stream_options: this.config.includeUsage ? {\n        include_usage: true\n      } : void 0\n    };\n    const metadataExtractor = (_a = this.config.metadataExtractor) == null ? void 0 : _a.createStreamExtractor();\n    const {\n      responseHeaders,\n      value: response\n    } = await postJsonToApi({\n      url: this.config.url({\n        path: \"/chat/completions\",\n        modelId: this.modelId\n      }),\n      headers: combineHeaders(this.config.headers(), options.headers),\n      body,\n      failedResponseHandler: this.failedResponseHandler,\n      successfulResponseHandler: createEventSourceResponseHandler(this.chunkSchema),\n      abortSignal: options.abortSignal,\n      fetch: this.config.fetch\n    });\n    const {\n      messages: rawPrompt,\n      ...rawSettings\n    } = args;\n    const toolCalls = [];\n    let finishReason = \"unknown\";\n    let usage = {\n      completionTokens: void 0,\n      completionTokensDetails: {\n        reasoningTokens: void 0,\n        acceptedPredictionTokens: void 0,\n        rejectedPredictionTokens: void 0\n      },\n      promptTokens: void 0,\n      promptTokensDetails: {\n        cachedTokens: void 0\n      }\n    };\n    let isFirstChunk = true;\n    let providerOptionsName = this.providerOptionsName;\n    return {\n      stream: response.pipeThrough(new TransformStream({\n        // TODO we lost type safety on Chunk, most likely due to the error schema. MUST FIX\n        transform(chunk, controller) {\n          var _a2, _b, _c, _d, _e, _f, _g, _h, _i, _j, _k, _l;\n          if (!chunk.success) {\n            finishReason = \"error\";\n            controller.enqueue({\n              type: \"error\",\n              error: chunk.error\n            });\n            return;\n          }\n          const value = chunk.value;\n          metadataExtractor == null ? void 0 : metadataExtractor.processChunk(chunk.rawValue);\n          if (\"error\" in value) {\n            finishReason = \"error\";\n            controller.enqueue({\n              type: \"error\",\n              error: value.error.message\n            });\n            return;\n          }\n          if (isFirstChunk) {\n            isFirstChunk = false;\n            controller.enqueue({\n              type: \"response-metadata\",\n              ...getResponseMetadata(value)\n            });\n          }\n          if (value.usage != null) {\n            const {\n              prompt_tokens,\n              completion_tokens,\n              prompt_tokens_details,\n              completion_tokens_details\n            } = value.usage;\n            usage.promptTokens = prompt_tokens != null ? prompt_tokens : void 0;\n            usage.completionTokens = completion_tokens != null ? completion_tokens : void 0;\n            if ((completion_tokens_details == null ? void 0 : completion_tokens_details.reasoning_tokens) != null) {\n              usage.completionTokensDetails.reasoningTokens = completion_tokens_details == null ? void 0 : completion_tokens_details.reasoning_tokens;\n            }\n            if ((completion_tokens_details == null ? void 0 : completion_tokens_details.accepted_prediction_tokens) != null) {\n              usage.completionTokensDetails.acceptedPredictionTokens = completion_tokens_details == null ? void 0 : completion_tokens_details.accepted_prediction_tokens;\n            }\n            if ((completion_tokens_details == null ? void 0 : completion_tokens_details.rejected_prediction_tokens) != null) {\n              usage.completionTokensDetails.rejectedPredictionTokens = completion_tokens_details == null ? void 0 : completion_tokens_details.rejected_prediction_tokens;\n            }\n            if ((prompt_tokens_details == null ? void 0 : prompt_tokens_details.cached_tokens) != null) {\n              usage.promptTokensDetails.cachedTokens = prompt_tokens_details == null ? void 0 : prompt_tokens_details.cached_tokens;\n            }\n          }\n          const choice = value.choices[0];\n          if ((choice == null ? void 0 : choice.finish_reason) != null) {\n            finishReason = mapOpenAICompatibleFinishReason(choice.finish_reason);\n          }\n          if ((choice == null ? void 0 : choice.delta) == null) {\n            return;\n          }\n          const delta = choice.delta;\n          if (delta.reasoning_content != null) {\n            controller.enqueue({\n              type: \"reasoning\",\n              textDelta: delta.reasoning_content\n            });\n          }\n          if (delta.content != null) {\n            controller.enqueue({\n              type: \"text-delta\",\n              textDelta: delta.content\n            });\n          }\n          if (delta.tool_calls != null) {\n            for (const toolCallDelta of delta.tool_calls) {\n              const index = toolCallDelta.index;\n              if (toolCalls[index] == null) {\n                if (toolCallDelta.type !== \"function\") {\n                  throw new InvalidResponseDataError({\n                    data: toolCallDelta,\n                    message: `Expected 'function' type.`\n                  });\n                }\n                if (toolCallDelta.id == null) {\n                  throw new InvalidResponseDataError({\n                    data: toolCallDelta,\n                    message: `Expected 'id' to be a string.`\n                  });\n                }\n                if (((_a2 = toolCallDelta.function) == null ? void 0 : _a2.name) == null) {\n                  throw new InvalidResponseDataError({\n                    data: toolCallDelta,\n                    message: `Expected 'function.name' to be a string.`\n                  });\n                }\n                toolCalls[index] = {\n                  id: toolCallDelta.id,\n                  type: \"function\",\n                  function: {\n                    name: toolCallDelta.function.name,\n                    arguments: (_b = toolCallDelta.function.arguments) != null ? _b : \"\"\n                  },\n                  hasFinished: false\n                };\n                const toolCall2 = toolCalls[index];\n                if (((_c = toolCall2.function) == null ? void 0 : _c.name) != null && ((_d = toolCall2.function) == null ? void 0 : _d.arguments) != null) {\n                  if (toolCall2.function.arguments.length > 0) {\n                    controller.enqueue({\n                      type: \"tool-call-delta\",\n                      toolCallType: \"function\",\n                      toolCallId: toolCall2.id,\n                      toolName: toolCall2.function.name,\n                      argsTextDelta: toolCall2.function.arguments\n                    });\n                  }\n                  if (isParsableJson(toolCall2.function.arguments)) {\n                    controller.enqueue({\n                      type: \"tool-call\",\n                      toolCallType: \"function\",\n                      toolCallId: (_e = toolCall2.id) != null ? _e : generateId(),\n                      toolName: toolCall2.function.name,\n                      args: toolCall2.function.arguments\n                    });\n                    toolCall2.hasFinished = true;\n                  }\n                }\n                continue;\n              }\n              const toolCall = toolCalls[index];\n              if (toolCall.hasFinished) {\n                continue;\n              }\n              if (((_f = toolCallDelta.function) == null ? void 0 : _f.arguments) != null) {\n                toolCall.function.arguments += (_h = (_g = toolCallDelta.function) == null ? void 0 : _g.arguments) != null ? _h : \"\";\n              }\n              controller.enqueue({\n                type: \"tool-call-delta\",\n                toolCallType: \"function\",\n                toolCallId: toolCall.id,\n                toolName: toolCall.function.name,\n                argsTextDelta: (_i = toolCallDelta.function.arguments) != null ? _i : \"\"\n              });\n              if (((_j = toolCall.function) == null ? void 0 : _j.name) != null && ((_k = toolCall.function) == null ? void 0 : _k.arguments) != null && isParsableJson(toolCall.function.arguments)) {\n                controller.enqueue({\n                  type: \"tool-call\",\n                  toolCallType: \"function\",\n                  toolCallId: (_l = toolCall.id) != null ? _l : generateId(),\n                  toolName: toolCall.function.name,\n                  args: toolCall.function.arguments\n                });\n                toolCall.hasFinished = true;\n              }\n            }\n          }\n        },\n        flush(controller) {\n          var _a2, _b;\n          const providerMetadata = {\n            [providerOptionsName]: {},\n            ...(metadataExtractor == null ? void 0 : metadataExtractor.buildMetadata())\n          };\n          if (usage.completionTokensDetails.reasoningTokens != null) {\n            providerMetadata[providerOptionsName].reasoningTokens = usage.completionTokensDetails.reasoningTokens;\n          }\n          if (usage.completionTokensDetails.acceptedPredictionTokens != null) {\n            providerMetadata[providerOptionsName].acceptedPredictionTokens = usage.completionTokensDetails.acceptedPredictionTokens;\n          }\n          if (usage.completionTokensDetails.rejectedPredictionTokens != null) {\n            providerMetadata[providerOptionsName].rejectedPredictionTokens = usage.completionTokensDetails.rejectedPredictionTokens;\n          }\n          if (usage.promptTokensDetails.cachedTokens != null) {\n            providerMetadata[providerOptionsName].cachedPromptTokens = usage.promptTokensDetails.cachedTokens;\n          }\n          controller.enqueue({\n            type: \"finish\",\n            finishReason,\n            usage: {\n              promptTokens: (_a2 = usage.promptTokens) != null ? _a2 : NaN,\n              completionTokens: (_b = usage.completionTokens) != null ? _b : NaN\n            },\n            providerMetadata\n          });\n        }\n      })),\n      rawCall: {\n        rawPrompt,\n        rawSettings\n      },\n      rawResponse: {\n        headers: responseHeaders\n      },\n      warnings,\n      request: {\n        body: JSON.stringify(body)\n      }\n    };\n  }\n};\nvar openaiCompatibleTokenUsageSchema = z2.object({\n  prompt_tokens: z2.number().nullish(),\n  completion_tokens: z2.number().nullish(),\n  prompt_tokens_details: z2.object({\n    cached_tokens: z2.number().nullish()\n  }).nullish(),\n  completion_tokens_details: z2.object({\n    reasoning_tokens: z2.number().nullish(),\n    accepted_prediction_tokens: z2.number().nullish(),\n    rejected_prediction_tokens: z2.number().nullish()\n  }).nullish()\n}).nullish();\nvar OpenAICompatibleChatResponseSchema = z2.object({\n  id: z2.string().nullish(),\n  created: z2.number().nullish(),\n  model: z2.string().nullish(),\n  choices: z2.array(z2.object({\n    message: z2.object({\n      role: z2.literal(\"assistant\").nullish(),\n      content: z2.string().nullish(),\n      reasoning_content: z2.string().nullish(),\n      tool_calls: z2.array(z2.object({\n        id: z2.string().nullish(),\n        type: z2.literal(\"function\"),\n        function: z2.object({\n          name: z2.string(),\n          arguments: z2.string()\n        })\n      })).nullish()\n    }),\n    finish_reason: z2.string().nullish()\n  })),\n  usage: openaiCompatibleTokenUsageSchema\n});\nvar createOpenAICompatibleChatChunkSchema = errorSchema => z2.union([z2.object({\n  id: z2.string().nullish(),\n  created: z2.number().nullish(),\n  model: z2.string().nullish(),\n  choices: z2.array(z2.object({\n    delta: z2.object({\n      role: z2.enum([\"assistant\"]).nullish(),\n      content: z2.string().nullish(),\n      reasoning_content: z2.string().nullish(),\n      tool_calls: z2.array(z2.object({\n        index: z2.number().optional(),\n        id: z2.string().nullish(),\n        type: z2.literal(\"function\").nullish(),\n        function: z2.object({\n          name: z2.string().nullish(),\n          arguments: z2.string().nullish()\n        })\n      })).nullish()\n    }).nullish(),\n    finish_reason: z2.string().nullish()\n  })),\n  usage: openaiCompatibleTokenUsageSchema\n}), errorSchema]);\n\n// src/openai-compatible-completion-language-model.ts\nimport { UnsupportedFunctionalityError as UnsupportedFunctionalityError4 } from \"@ai-sdk/provider\";\nimport { combineHeaders as combineHeaders2, createEventSourceResponseHandler as createEventSourceResponseHandler2, createJsonErrorResponseHandler as createJsonErrorResponseHandler2, createJsonResponseHandler as createJsonResponseHandler2, postJsonToApi as postJsonToApi2 } from \"@ai-sdk/provider-utils\";\nimport { z as z3 } from \"zod\";\n\n// src/convert-to-openai-compatible-completion-prompt.ts\nimport { InvalidPromptError, UnsupportedFunctionalityError as UnsupportedFunctionalityError3 } from \"@ai-sdk/provider\";\nfunction convertToOpenAICompatibleCompletionPrompt({\n  prompt,\n  inputFormat,\n  user = \"user\",\n  assistant = \"assistant\"\n}) {\n  if (inputFormat === \"prompt\" && prompt.length === 1 && prompt[0].role === \"user\" && prompt[0].content.length === 1 && prompt[0].content[0].type === \"text\") {\n    return {\n      prompt: prompt[0].content[0].text\n    };\n  }\n  let text = \"\";\n  if (prompt[0].role === \"system\") {\n    text += `${prompt[0].content}\n\n`;\n    prompt = prompt.slice(1);\n  }\n  for (const {\n    role,\n    content\n  } of prompt) {\n    switch (role) {\n      case \"system\":\n        {\n          throw new InvalidPromptError({\n            message: \"Unexpected system message in prompt: ${content}\",\n            prompt\n          });\n        }\n      case \"user\":\n        {\n          const userMessage = content.map(part => {\n            switch (part.type) {\n              case \"text\":\n                {\n                  return part.text;\n                }\n              case \"image\":\n                {\n                  throw new UnsupportedFunctionalityError3({\n                    functionality: \"images\"\n                  });\n                }\n            }\n          }).join(\"\");\n          text += `${user}:\n${userMessage}\n\n`;\n          break;\n        }\n      case \"assistant\":\n        {\n          const assistantMessage = content.map(part => {\n            switch (part.type) {\n              case \"text\":\n                {\n                  return part.text;\n                }\n              case \"tool-call\":\n                {\n                  throw new UnsupportedFunctionalityError3({\n                    functionality: \"tool-call messages\"\n                  });\n                }\n            }\n          }).join(\"\");\n          text += `${assistant}:\n${assistantMessage}\n\n`;\n          break;\n        }\n      case \"tool\":\n        {\n          throw new UnsupportedFunctionalityError3({\n            functionality: \"tool messages\"\n          });\n        }\n      default:\n        {\n          const _exhaustiveCheck = role;\n          throw new Error(`Unsupported role: ${_exhaustiveCheck}`);\n        }\n    }\n  }\n  text += `${assistant}:\n`;\n  return {\n    prompt: text,\n    stopSequences: [`\n${user}:`]\n  };\n}\n\n// src/openai-compatible-completion-language-model.ts\nvar OpenAICompatibleCompletionLanguageModel = class {\n  // type inferred via constructor\n  constructor(modelId, settings, config) {\n    this.specificationVersion = \"v1\";\n    this.defaultObjectGenerationMode = void 0;\n    var _a;\n    this.modelId = modelId;\n    this.settings = settings;\n    this.config = config;\n    const errorStructure = (_a = config.errorStructure) != null ? _a : defaultOpenAICompatibleErrorStructure;\n    this.chunkSchema = createOpenAICompatibleCompletionChunkSchema(errorStructure.errorSchema);\n    this.failedResponseHandler = createJsonErrorResponseHandler2(errorStructure);\n  }\n  get provider() {\n    return this.config.provider;\n  }\n  get providerOptionsName() {\n    return this.config.provider.split(\".\")[0].trim();\n  }\n  getArgs({\n    mode,\n    inputFormat,\n    prompt,\n    maxTokens,\n    temperature,\n    topP,\n    topK,\n    frequencyPenalty,\n    presencePenalty,\n    stopSequences: userStopSequences,\n    responseFormat,\n    seed,\n    providerMetadata\n  }) {\n    var _a;\n    const type = mode.type;\n    const warnings = [];\n    if (topK != null) {\n      warnings.push({\n        type: \"unsupported-setting\",\n        setting: \"topK\"\n      });\n    }\n    if (responseFormat != null && responseFormat.type !== \"text\") {\n      warnings.push({\n        type: \"unsupported-setting\",\n        setting: \"responseFormat\",\n        details: \"JSON response format is not supported.\"\n      });\n    }\n    const {\n      prompt: completionPrompt,\n      stopSequences\n    } = convertToOpenAICompatibleCompletionPrompt({\n      prompt,\n      inputFormat\n    });\n    const stop = [...(stopSequences != null ? stopSequences : []), ...(userStopSequences != null ? userStopSequences : [])];\n    const baseArgs = {\n      // model id:\n      model: this.modelId,\n      // model specific settings:\n      echo: this.settings.echo,\n      logit_bias: this.settings.logitBias,\n      suffix: this.settings.suffix,\n      user: this.settings.user,\n      // standardized settings:\n      max_tokens: maxTokens,\n      temperature,\n      top_p: topP,\n      frequency_penalty: frequencyPenalty,\n      presence_penalty: presencePenalty,\n      seed,\n      ...(providerMetadata == null ? void 0 : providerMetadata[this.providerOptionsName]),\n      // prompt:\n      prompt: completionPrompt,\n      // stop sequences:\n      stop: stop.length > 0 ? stop : void 0\n    };\n    switch (type) {\n      case \"regular\":\n        {\n          if ((_a = mode.tools) == null ? void 0 : _a.length) {\n            throw new UnsupportedFunctionalityError4({\n              functionality: \"tools\"\n            });\n          }\n          if (mode.toolChoice) {\n            throw new UnsupportedFunctionalityError4({\n              functionality: \"toolChoice\"\n            });\n          }\n          return {\n            args: baseArgs,\n            warnings\n          };\n        }\n      case \"object-json\":\n        {\n          throw new UnsupportedFunctionalityError4({\n            functionality: \"object-json mode\"\n          });\n        }\n      case \"object-tool\":\n        {\n          throw new UnsupportedFunctionalityError4({\n            functionality: \"object-tool mode\"\n          });\n        }\n      default:\n        {\n          const _exhaustiveCheck = type;\n          throw new Error(`Unsupported type: ${_exhaustiveCheck}`);\n        }\n    }\n  }\n  async doGenerate(options) {\n    var _a, _b, _c, _d;\n    const {\n      args,\n      warnings\n    } = this.getArgs(options);\n    const {\n      responseHeaders,\n      value: response,\n      rawValue: rawResponse\n    } = await postJsonToApi2({\n      url: this.config.url({\n        path: \"/completions\",\n        modelId: this.modelId\n      }),\n      headers: combineHeaders2(this.config.headers(), options.headers),\n      body: args,\n      failedResponseHandler: this.failedResponseHandler,\n      successfulResponseHandler: createJsonResponseHandler2(openaiCompatibleCompletionResponseSchema),\n      abortSignal: options.abortSignal,\n      fetch: this.config.fetch\n    });\n    const {\n      prompt: rawPrompt,\n      ...rawSettings\n    } = args;\n    const choice = response.choices[0];\n    return {\n      text: choice.text,\n      usage: {\n        promptTokens: (_b = (_a = response.usage) == null ? void 0 : _a.prompt_tokens) != null ? _b : NaN,\n        completionTokens: (_d = (_c = response.usage) == null ? void 0 : _c.completion_tokens) != null ? _d : NaN\n      },\n      finishReason: mapOpenAICompatibleFinishReason(choice.finish_reason),\n      rawCall: {\n        rawPrompt,\n        rawSettings\n      },\n      rawResponse: {\n        headers: responseHeaders,\n        body: rawResponse\n      },\n      response: getResponseMetadata(response),\n      warnings,\n      request: {\n        body: JSON.stringify(args)\n      }\n    };\n  }\n  async doStream(options) {\n    const {\n      args,\n      warnings\n    } = this.getArgs(options);\n    const body = {\n      ...args,\n      stream: true,\n      // only include stream_options when in strict compatibility mode:\n      stream_options: this.config.includeUsage ? {\n        include_usage: true\n      } : void 0\n    };\n    const {\n      responseHeaders,\n      value: response\n    } = await postJsonToApi2({\n      url: this.config.url({\n        path: \"/completions\",\n        modelId: this.modelId\n      }),\n      headers: combineHeaders2(this.config.headers(), options.headers),\n      body,\n      failedResponseHandler: this.failedResponseHandler,\n      successfulResponseHandler: createEventSourceResponseHandler2(this.chunkSchema),\n      abortSignal: options.abortSignal,\n      fetch: this.config.fetch\n    });\n    const {\n      prompt: rawPrompt,\n      ...rawSettings\n    } = args;\n    let finishReason = \"unknown\";\n    let usage = {\n      promptTokens: Number.NaN,\n      completionTokens: Number.NaN\n    };\n    let isFirstChunk = true;\n    return {\n      stream: response.pipeThrough(new TransformStream({\n        transform(chunk, controller) {\n          if (!chunk.success) {\n            finishReason = \"error\";\n            controller.enqueue({\n              type: \"error\",\n              error: chunk.error\n            });\n            return;\n          }\n          const value = chunk.value;\n          if (\"error\" in value) {\n            finishReason = \"error\";\n            controller.enqueue({\n              type: \"error\",\n              error: value.error\n            });\n            return;\n          }\n          if (isFirstChunk) {\n            isFirstChunk = false;\n            controller.enqueue({\n              type: \"response-metadata\",\n              ...getResponseMetadata(value)\n            });\n          }\n          if (value.usage != null) {\n            usage = {\n              promptTokens: value.usage.prompt_tokens,\n              completionTokens: value.usage.completion_tokens\n            };\n          }\n          const choice = value.choices[0];\n          if ((choice == null ? void 0 : choice.finish_reason) != null) {\n            finishReason = mapOpenAICompatibleFinishReason(choice.finish_reason);\n          }\n          if ((choice == null ? void 0 : choice.text) != null) {\n            controller.enqueue({\n              type: \"text-delta\",\n              textDelta: choice.text\n            });\n          }\n        },\n        flush(controller) {\n          controller.enqueue({\n            type: \"finish\",\n            finishReason,\n            usage\n          });\n        }\n      })),\n      rawCall: {\n        rawPrompt,\n        rawSettings\n      },\n      rawResponse: {\n        headers: responseHeaders\n      },\n      warnings,\n      request: {\n        body: JSON.stringify(body)\n      }\n    };\n  }\n};\nvar openaiCompatibleCompletionResponseSchema = z3.object({\n  id: z3.string().nullish(),\n  created: z3.number().nullish(),\n  model: z3.string().nullish(),\n  choices: z3.array(z3.object({\n    text: z3.string(),\n    finish_reason: z3.string()\n  })),\n  usage: z3.object({\n    prompt_tokens: z3.number(),\n    completion_tokens: z3.number()\n  }).nullish()\n});\nvar createOpenAICompatibleCompletionChunkSchema = errorSchema => z3.union([z3.object({\n  id: z3.string().nullish(),\n  created: z3.number().nullish(),\n  model: z3.string().nullish(),\n  choices: z3.array(z3.object({\n    text: z3.string(),\n    finish_reason: z3.string().nullish(),\n    index: z3.number()\n  })),\n  usage: z3.object({\n    prompt_tokens: z3.number(),\n    completion_tokens: z3.number()\n  }).nullish()\n}), errorSchema]);\n\n// src/openai-compatible-embedding-model.ts\nimport { TooManyEmbeddingValuesForCallError } from \"@ai-sdk/provider\";\nimport { combineHeaders as combineHeaders3, createJsonErrorResponseHandler as createJsonErrorResponseHandler3, createJsonResponseHandler as createJsonResponseHandler3, postJsonToApi as postJsonToApi3 } from \"@ai-sdk/provider-utils\";\nimport { z as z4 } from \"zod\";\nvar OpenAICompatibleEmbeddingModel = class {\n  constructor(modelId, settings, config) {\n    this.specificationVersion = \"v1\";\n    this.modelId = modelId;\n    this.settings = settings;\n    this.config = config;\n  }\n  get provider() {\n    return this.config.provider;\n  }\n  get maxEmbeddingsPerCall() {\n    var _a;\n    return (_a = this.config.maxEmbeddingsPerCall) != null ? _a : 2048;\n  }\n  get supportsParallelCalls() {\n    var _a;\n    return (_a = this.config.supportsParallelCalls) != null ? _a : true;\n  }\n  async doEmbed({\n    values,\n    headers,\n    abortSignal\n  }) {\n    var _a;\n    if (values.length > this.maxEmbeddingsPerCall) {\n      throw new TooManyEmbeddingValuesForCallError({\n        provider: this.provider,\n        modelId: this.modelId,\n        maxEmbeddingsPerCall: this.maxEmbeddingsPerCall,\n        values\n      });\n    }\n    const {\n      responseHeaders,\n      value: response\n    } = await postJsonToApi3({\n      url: this.config.url({\n        path: \"/embeddings\",\n        modelId: this.modelId\n      }),\n      headers: combineHeaders3(this.config.headers(), headers),\n      body: {\n        model: this.modelId,\n        input: values,\n        encoding_format: \"float\",\n        dimensions: this.settings.dimensions,\n        user: this.settings.user\n      },\n      failedResponseHandler: createJsonErrorResponseHandler3((_a = this.config.errorStructure) != null ? _a : defaultOpenAICompatibleErrorStructure),\n      successfulResponseHandler: createJsonResponseHandler3(openaiTextEmbeddingResponseSchema),\n      abortSignal,\n      fetch: this.config.fetch\n    });\n    return {\n      embeddings: response.data.map(item => item.embedding),\n      usage: response.usage ? {\n        tokens: response.usage.prompt_tokens\n      } : void 0,\n      rawResponse: {\n        headers: responseHeaders\n      }\n    };\n  }\n};\nvar openaiTextEmbeddingResponseSchema = z4.object({\n  data: z4.array(z4.object({\n    embedding: z4.array(z4.number())\n  })),\n  usage: z4.object({\n    prompt_tokens: z4.number()\n  }).nullish()\n});\n\n// src/openai-compatible-image-model.ts\nimport { combineHeaders as combineHeaders4, createJsonErrorResponseHandler as createJsonErrorResponseHandler4, createJsonResponseHandler as createJsonResponseHandler4, postJsonToApi as postJsonToApi4 } from \"@ai-sdk/provider-utils\";\nimport { z as z5 } from \"zod\";\nvar OpenAICompatibleImageModel = class {\n  constructor(modelId, settings, config) {\n    this.modelId = modelId;\n    this.settings = settings;\n    this.config = config;\n    this.specificationVersion = \"v1\";\n  }\n  get maxImagesPerCall() {\n    var _a;\n    return (_a = this.settings.maxImagesPerCall) != null ? _a : 10;\n  }\n  get provider() {\n    return this.config.provider;\n  }\n  async doGenerate({\n    prompt,\n    n,\n    size,\n    aspectRatio,\n    seed,\n    providerOptions,\n    headers,\n    abortSignal\n  }) {\n    var _a, _b, _c, _d, _e;\n    const warnings = [];\n    if (aspectRatio != null) {\n      warnings.push({\n        type: \"unsupported-setting\",\n        setting: \"aspectRatio\",\n        details: \"This model does not support aspect ratio. Use `size` instead.\"\n      });\n    }\n    if (seed != null) {\n      warnings.push({\n        type: \"unsupported-setting\",\n        setting: \"seed\"\n      });\n    }\n    const currentDate = (_c = (_b = (_a = this.config._internal) == null ? void 0 : _a.currentDate) == null ? void 0 : _b.call(_a)) != null ? _c : /* @__PURE__ */new Date();\n    const {\n      value: response,\n      responseHeaders\n    } = await postJsonToApi4({\n      url: this.config.url({\n        path: \"/images/generations\",\n        modelId: this.modelId\n      }),\n      headers: combineHeaders4(this.config.headers(), headers),\n      body: {\n        model: this.modelId,\n        prompt,\n        n,\n        size,\n        ...((_d = providerOptions.openai) != null ? _d : {}),\n        response_format: \"b64_json\",\n        ...(this.settings.user ? {\n          user: this.settings.user\n        } : {})\n      },\n      failedResponseHandler: createJsonErrorResponseHandler4((_e = this.config.errorStructure) != null ? _e : defaultOpenAICompatibleErrorStructure),\n      successfulResponseHandler: createJsonResponseHandler4(openaiCompatibleImageResponseSchema),\n      abortSignal,\n      fetch: this.config.fetch\n    });\n    return {\n      images: response.data.map(item => item.b64_json),\n      warnings,\n      response: {\n        timestamp: currentDate,\n        modelId: this.modelId,\n        headers: responseHeaders\n      }\n    };\n  }\n};\nvar openaiCompatibleImageResponseSchema = z5.object({\n  data: z5.array(z5.object({\n    b64_json: z5.string()\n  }))\n});\n\n// src/openai-compatible-provider.ts\nimport { withoutTrailingSlash } from \"@ai-sdk/provider-utils\";\nfunction createOpenAICompatible(options) {\n  const baseURL = withoutTrailingSlash(options.baseURL);\n  const providerName = options.name;\n  const getHeaders = () => ({\n    ...(options.apiKey && {\n      Authorization: `Bearer ${options.apiKey}`\n    }),\n    ...options.headers\n  });\n  const getCommonModelConfig = modelType => ({\n    provider: `${providerName}.${modelType}`,\n    url: ({\n      path\n    }) => {\n      const url = new URL(`${baseURL}${path}`);\n      if (options.queryParams) {\n        url.search = new URLSearchParams(options.queryParams).toString();\n      }\n      return url.toString();\n    },\n    headers: getHeaders,\n    fetch: options.fetch\n  });\n  const createLanguageModel = (modelId, settings = {}, config) => createChatModel(modelId, settings, config);\n  const createChatModel = (modelId, settings = {}, config) => new OpenAICompatibleChatLanguageModel(modelId, settings, {\n    ...getCommonModelConfig(\"chat\"),\n    defaultObjectGenerationMode: \"tool\",\n    ...config\n  });\n  const createCompletionModel = (modelId, settings = {}) => new OpenAICompatibleCompletionLanguageModel(modelId, settings, getCommonModelConfig(\"completion\"));\n  const createEmbeddingModel = (modelId, settings = {}) => new OpenAICompatibleEmbeddingModel(modelId, settings, getCommonModelConfig(\"embedding\"));\n  const createImageModel = (modelId, settings = {}) => new OpenAICompatibleImageModel(modelId, settings, getCommonModelConfig(\"image\"));\n  const provider = (modelId, settings, config) => createLanguageModel(modelId, settings, config);\n  provider.languageModel = createLanguageModel;\n  provider.chatModel = createChatModel;\n  provider.completionModel = createCompletionModel;\n  provider.textEmbeddingModel = createEmbeddingModel;\n  provider.imageModel = createImageModel;\n  return provider;\n}\nexport { OpenAICompatibleChatLanguageModel, OpenAICompatibleCompletionLanguageModel, OpenAICompatibleEmbeddingModel, OpenAICompatibleImageModel, createOpenAICompatible };\n//# sourceMappingURL=index.mjs.map","map":null,"metadata":{},"sourceType":"module","externalDependencies":[]}