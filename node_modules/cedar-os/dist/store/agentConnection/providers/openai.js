"use strict";
var __defProp = Object.defineProperty;
var __getOwnPropDesc = Object.getOwnPropertyDescriptor;
var __getOwnPropNames = Object.getOwnPropertyNames;
var __hasOwnProp = Object.prototype.hasOwnProperty;
var __export = (target, all) => {
  for (var name in all)
    __defProp(target, name, { get: all[name], enumerable: true });
};
var __copyProps = (to, from, except, desc) => {
  if (from && typeof from === "object" || typeof from === "function") {
    for (let key of __getOwnPropNames(from))
      if (!__hasOwnProp.call(to, key) && key !== except)
        __defProp(to, key, { get: () => from[key], enumerable: !(desc = __getOwnPropDesc(from, key)) || desc.enumerable });
  }
  return to;
};
var __toCommonJS = (mod) => __copyProps(__defProp({}, "__esModule", { value: true }), mod);

// src/store/agentConnection/providers/openai.ts
var openai_exports = {};
__export(openai_exports, {
  openAIProvider: () => openAIProvider
});
module.exports = __toCommonJS(openai_exports);

// src/store/agentConnection/agentUtils.ts
var processContentChunk = (rawChunk) => {
  return rawChunk.replace(/(\\n|\n)/g, "\n");
};
async function handleEventStream(response, handler) {
  if (!response.ok || !response.body) {
    throw new Error(`HTTP error! status: ${response.status}`);
  }
  const reader = response.body.getReader();
  const decoder = new TextDecoder();
  let buffer = "";
  const completedItems = [];
  let currentTextMessage = "";
  const parseSSEEvent = (raw) => {
    let eventType = "message";
    let data = "";
    for (const line of raw.split("\n")) {
      if (line.startsWith("event:")) {
        eventType = line.slice(6).trim();
      } else if (line.startsWith("data:")) {
        data += line.slice(5);
      }
    }
    return { eventType, data };
  };
  const processDataContent = (data) => {
    if (data.trim() === "[DONE]" || data.trim() === "done") {
      return;
    }
    try {
      const parsed = JSON.parse(data);
      if (parsed === null || typeof parsed !== "object") {
        const processedContent = processContentChunk(String(parsed));
        currentTextMessage += processedContent;
        handler({ type: "chunk", content: processedContent });
        return;
      }
      if (parsed.choices && parsed.choices[0] && parsed.choices[0].delta) {
        const delta = parsed.choices[0].delta;
        if (delta.content) {
          const processedContent = processContentChunk(delta.content);
          currentTextMessage += processedContent;
          handler({ type: "chunk", content: processedContent });
        }
        if (delta.tool_calls || delta.function_call) {
          if (currentTextMessage.trim()) {
            completedItems.push(currentTextMessage.trim());
            currentTextMessage = "";
          }
          handler({ type: "object", object: delta });
          completedItems.push(delta);
        }
        if (Object.keys(delta).length === 0) {
          return;
        }
      }
      if (typeof parsed.content === "string" && parsed.content.length > 0) {
        const processedContent = processContentChunk(parsed.content);
        currentTextMessage += processedContent;
        handler({ type: "chunk", content: processedContent });
      }
      if (parsed.type || parsed.object && parsed.object.type) {
        const structuredObject = parsed.type ? parsed : parsed.object;
        if (currentTextMessage.trim()) {
          completedItems.push(currentTextMessage.trim());
          currentTextMessage = "";
        }
        handler({ type: "object", object: structuredObject });
        completedItems.push(structuredObject);
      }
      if (!parsed.choices && !parsed.type && !(parsed.object && parsed.object.type) && !parsed.content) {
        if (currentTextMessage.trim()) {
          completedItems.push(currentTextMessage.trim());
          currentTextMessage = "";
        }
        handler({ type: "object", object: parsed });
        completedItems.push(parsed);
      }
    } catch {
      if (data && data !== "[DONE]" && data !== "done") {
        const processedContent = processContentChunk(data);
        currentTextMessage += processedContent;
        handler({ type: "chunk", content: processedContent });
      }
    }
  };
  try {
    while (true) {
      const { value, done } = await reader.read();
      if (done) break;
      buffer += decoder.decode(value, { stream: true });
      let eventBoundary;
      while ((eventBoundary = buffer.indexOf("\n\n")) !== -1) {
        const rawEvent = buffer.slice(0, eventBoundary);
        buffer = buffer.slice(eventBoundary + 2);
        if (!rawEvent.trim()) continue;
        const { eventType, data } = parseSSEEvent(rawEvent);
        if (eventType.trim() === "done" || data.trim() === "[DONE]") {
          break;
        } else {
          processDataContent(data);
        }
      }
    }
    if (currentTextMessage.trim()) {
      completedItems.push(currentTextMessage.trim());
    }
    handler({ type: "done", completedItems });
  } catch (error) {
    handler({ type: "error", error });
    throw error;
  }
}

// src/store/agentConnection/providers/openai.ts
var openAIProvider = {
  callLLM: async (params, config) => {
    const {
      prompt,
      model,
      systemPrompt,
      temperature,
      maxTokens,
      messages: providedMessages,
      ...rest
    } = params;
    const messages = providedMessages || [
      ...systemPrompt ? [{ role: "system", content: systemPrompt }] : [],
      { role: "user", content: prompt || "" }
    ];
    const response = await fetch("https://api.openai.com/v1/chat/completions", {
      method: "POST",
      headers: {
        "Content-Type": "application/json",
        Authorization: `Bearer ${config.apiKey}`
      },
      body: JSON.stringify({
        model,
        messages,
        temperature,
        max_tokens: maxTokens,
        ...rest
      })
    });
    return openAIProvider.handleResponse(response);
  },
  streamLLM: (params, config, handler) => {
    const abortController = new AbortController();
    const completion = (async () => {
      try {
        const {
          prompt,
          model,
          systemPrompt,
          temperature,
          maxTokens,
          messages: providedMessages,
          ...rest
        } = params;
        const messages = providedMessages || [
          ...systemPrompt ? [{ role: "system", content: systemPrompt }] : [],
          { role: "user", content: prompt || "" }
        ];
        const response = await fetch(
          "https://api.openai.com/v1/chat/completions",
          {
            method: "POST",
            headers: {
              "Content-Type": "application/json",
              Authorization: `Bearer ${config.apiKey}`
            },
            body: JSON.stringify({
              model,
              messages,
              temperature,
              max_tokens: maxTokens,
              stream: true,
              ...rest
            }),
            signal: abortController.signal
          }
        );
        if (!response.ok) {
          throw new Error(`HTTP error! status: ${response.status}`);
        }
        await handleEventStream(response, handler);
      } catch (error) {
        if (error instanceof Error && error.name !== "AbortError") {
          handler({ type: "error", error });
        }
      }
    })();
    return {
      abort: () => abortController.abort(),
      completion
    };
  },
  handleResponse: async (response) => {
    if (!response.ok) {
      throw new Error(`HTTP error! status: ${response.status}`);
    }
    const data = await response.json();
    return {
      content: data.choices?.[0]?.message?.content || "",
      usage: data.usage ? {
        promptTokens: data.usage.prompt_tokens,
        completionTokens: data.usage.completion_tokens,
        totalTokens: data.usage.total_tokens
      } : void 0,
      metadata: {
        model: data.model,
        id: data.id
      }
    };
  },
  callLLMStructured: async (params, config) => {
    const {
      prompt,
      model,
      systemPrompt,
      temperature,
      maxTokens,
      schema,
      schemaName,
      schemaDescription,
      ...rest
    } = params;
    const messages = [
      ...systemPrompt ? [{ role: "system", content: systemPrompt }] : [],
      { role: "user", content: prompt }
    ];
    const body = {
      model,
      messages,
      temperature,
      max_tokens: maxTokens,
      ...rest
    };
    if (schema) {
      body.response_format = {
        type: "json_schema",
        json_schema: {
          name: schemaName || "response",
          description: schemaDescription,
          schema,
          strict: true
        }
      };
    }
    const response = await fetch("https://api.openai.com/v1/chat/completions", {
      method: "POST",
      headers: {
        "Content-Type": "application/json",
        Authorization: `Bearer ${config.apiKey}`
      },
      body: JSON.stringify(body)
    });
    const result = await openAIProvider.handleResponse(response);
    if (schema && result.content) {
      try {
        result.object = JSON.parse(result.content);
      } catch {
      }
    }
    return result;
  },
  voiceLLM: async (params, config) => {
    const { audioData, voiceSettings, context } = params;
    const voiceEndpoint = voiceSettings.endpoint || "/voice";
    const fullUrl = voiceEndpoint.startsWith("http") ? voiceEndpoint : voiceEndpoint;
    const formData = new FormData();
    formData.append("audio", audioData, "recording.webm");
    formData.append("settings", JSON.stringify(voiceSettings));
    if (context) {
      formData.append("context", JSON.stringify(context));
    }
    const response = await fetch(fullUrl, {
      method: "POST",
      headers: {
        Authorization: `Bearer ${config.apiKey}`
      },
      body: formData
    });
    if (!response.ok) {
      throw new Error(`Voice endpoint returned ${response.status}`);
    }
    const contentType = response.headers.get("content-type");
    if (contentType?.includes("audio")) {
      const audioBuffer = await response.arrayBuffer();
      const base64 = btoa(String.fromCharCode(...new Uint8Array(audioBuffer)));
      return {
        content: "",
        audioData: base64,
        audioFormat: contentType
      };
    } else if (contentType?.includes("application/json")) {
      const data = await response.json();
      return {
        content: data.text || data.content || "",
        transcription: data.transcription,
        audioData: data.audioData,
        audioUrl: data.audioUrl,
        audioFormat: data.audioFormat,
        usage: data.usage,
        metadata: data.metadata,
        object: data.object
      };
    } else {
      const text = await response.text();
      return {
        content: text
      };
    }
  }
};
// Annotate the CommonJS export names for ESM import in node:
0 && (module.exports = {
  openAIProvider
});
//# sourceMappingURL=openai.js.map