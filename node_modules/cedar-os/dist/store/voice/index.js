"use strict";
var __defProp = Object.defineProperty;
var __getOwnPropDesc = Object.getOwnPropertyDescriptor;
var __getOwnPropNames = Object.getOwnPropertyNames;
var __hasOwnProp = Object.prototype.hasOwnProperty;
var __export = (target, all) => {
  for (var name in all)
    __defProp(target, name, { get: all[name], enumerable: true });
};
var __copyProps = (to, from, except, desc) => {
  if (from && typeof from === "object" || typeof from === "function") {
    for (let key of __getOwnPropNames(from))
      if (!__hasOwnProp.call(to, key) && key !== except)
        __defProp(to, key, { get: () => from[key], enumerable: !(desc = __getOwnPropDesc(from, key)) || desc.enumerable });
  }
  return to;
};
var __toCommonJS = (mod) => __copyProps(__defProp({}, "__esModule", { value: true }), mod);

// src/store/voice/index.ts
var voice_exports = {};
__export(voice_exports, {
  createVoiceSlice: () => createVoiceSlice
});
module.exports = __toCommonJS(voice_exports);

// src/store/voice/voiceSlice.ts
var initialVoiceState = {
  isVoiceEnabled: false,
  isListening: false,
  isSpeaking: false,
  voicePermissionStatus: "prompt",
  audioStream: null,
  audioContext: null,
  mediaRecorder: null,
  voiceError: null,
  voiceSettings: {
    language: "en-US",
    pitch: 1,
    rate: 1,
    volume: 1,
    useBrowserTTS: false,
    autoAddToMessages: true,
    // Default to true for automatic message integration
    stream: false
    // Default to non-streaming
  }
};
var createVoiceSlice = (set, get) => ({
  ...initialVoiceState,
  checkVoiceSupport: () => {
    if (typeof window === "undefined") return false;
    return !!(navigator.mediaDevices && typeof navigator.mediaDevices.getUserMedia === "function" && window.MediaRecorder && window.AudioContext);
  },
  requestVoicePermission: async () => {
    try {
      if (!get().checkVoiceSupport()) {
        set({
          voicePermissionStatus: "not-supported",
          voiceError: "Voice features are not supported in this browser"
        });
        return;
      }
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
      const audioContext = new AudioContext();
      set({
        audioStream: stream,
        audioContext,
        voicePermissionStatus: "granted",
        voiceError: null
      });
    } catch (error) {
      set({
        voicePermissionStatus: "denied",
        voiceError: error instanceof Error ? error.message : "Failed to get microphone permission"
      });
    }
  },
  startListening: async () => {
    const state = get();
    if (state.voicePermissionStatus !== "granted") {
      await get().requestVoicePermission();
      if (get().voicePermissionStatus !== "granted") {
        return;
      }
    }
    if (!state.audioStream) {
      set({ voiceError: "No audio stream available" });
      return;
    }
    try {
      const mediaRecorder = new MediaRecorder(state.audioStream, {
        mimeType: "audio/webm;codecs=opus"
      });
      const audioChunks = [];
      mediaRecorder.ondataavailable = (event) => {
        if (event.data.size > 0) {
          audioChunks.push(event.data);
        }
      };
      mediaRecorder.onstop = async () => {
        const audioBlob = new Blob(audioChunks, { type: "audio/webm" });
        const { voiceSettings } = get();
        if (voiceSettings.stream) {
          await get().streamAudioToEndpointStream(audioBlob);
        } else {
          await get().streamAudioToEndpoint(audioBlob);
        }
      };
      mediaRecorder.start();
      set({
        mediaRecorder,
        isListening: true,
        voiceError: null
      });
    } catch (error) {
      set({
        voiceError: error instanceof Error ? error.message : "Failed to start recording"
      });
    }
  },
  stopListening: () => {
    const { mediaRecorder } = get();
    if (mediaRecorder && mediaRecorder.state !== "inactive") {
      mediaRecorder.stop();
    }
    set({ isListening: false });
  },
  toggleVoice: () => {
    const { isListening } = get();
    if (isListening) {
      get().stopListening();
    } else {
      get().startListening();
    }
  },
  streamAudioToEndpoint: async (audioData) => {
    const { voiceSettings } = get();
    try {
      set({ isSpeaking: false });
      get().setIsProcessing(true);
      const providerConfig = get().providerConfig;
      if (!providerConfig) {
        throw new Error("No provider configured for voice");
      }
      if ((providerConfig.provider === "mastra" || providerConfig.provider === "custom") && !voiceSettings.endpoint) {
        throw new Error("Voice endpoint not configured");
      }
      const contextString = get().compileAdditionalContext();
      const response = await get().voiceLLM({
        audioData,
        voiceSettings,
        context: contextString,
        prompt: ""
      });
      await get().handleLLMVoice(response);
    } catch (error) {
      set({
        voiceError: error instanceof Error ? error.message : "Failed to process voice"
      });
      get().setIsProcessing(false);
    }
  },
  streamAudioToEndpointStream: async (audioData) => {
    const { voiceSettings } = get();
    try {
      set({ isSpeaking: false });
      get().setIsProcessing(true);
      const providerConfig = get().providerConfig;
      if (!providerConfig) {
        throw new Error("No provider configured for voice");
      }
      if ((providerConfig.provider === "mastra" || providerConfig.provider === "custom") && !voiceSettings.endpoint) {
        throw new Error("Voice endpoint not configured");
      }
      const contextString = get().compileAdditionalContext();
      const streamResponse = get().voiceStreamLLM(
        {
          audioData,
          voiceSettings,
          context: contextString,
          prompt: ""
        },
        async (event) => {
          switch (event.type) {
            case "transcription":
              if (voiceSettings.autoAddToMessages && event.transcription) {
                const { addMessage } = get();
                addMessage({
                  type: "text",
                  role: "user",
                  content: event.transcription,
                  metadata: {
                    source: "voice",
                    timestamp: (/* @__PURE__ */ new Date()).toISOString()
                  }
                });
              }
              break;
            case "audio":
              if (event.audioData && event.audioFormat) {
                const binaryString = atob(event.audioData);
                const bytes = new Uint8Array(binaryString.length);
                for (let i = 0; i < binaryString.length; i++) {
                  bytes[i] = binaryString.charCodeAt(i);
                }
                const audioBuffer = bytes.buffer;
                await get().playAudioResponse(audioBuffer);
              }
              if (event.content) {
                get().addMessage({
                  type: "text",
                  role: "bot",
                  content: event.content,
                  metadata: {
                    source: "voice",
                    timestamp: (/* @__PURE__ */ new Date()).toISOString()
                  }
                });
              }
              break;
            case "chunk":
              await get().handleLLMResponse([event.content]);
              break;
            case "object":
              await get().handleLLMResponse(
                Array.isArray(event.object) ? event.object : [event.object]
              );
              break;
            case "done":
              get().setIsProcessing(false);
              break;
            case "error":
              console.error("Voice stream error:", event.error);
              set({
                voiceError: event.error instanceof Error ? event.error.message : "Voice stream error"
              });
              get().setIsProcessing(false);
              break;
          }
        }
      );
      await streamResponse.completion;
    } catch (error) {
      set({
        voiceError: error instanceof Error ? error.message : "Failed to process voice stream"
      });
      get().setIsProcessing(false);
    }
  },
  handleLLMVoice: async (response) => {
    const { voiceSettings } = get();
    try {
      set({ isSpeaking: false });
      if (response.audioData && response.audioFormat) {
        const binaryString = atob(response.audioData);
        const bytes = new Uint8Array(binaryString.length);
        for (let i = 0; i < binaryString.length; i++) {
          bytes[i] = binaryString.charCodeAt(i);
        }
        const audioBuffer = bytes.buffer;
        await get().playAudioResponse(audioBuffer);
      } else if (response.audioUrl) {
        await get().playAudioResponse(response.audioUrl);
      } else if (response.content && voiceSettings.useBrowserTTS) {
        if ("speechSynthesis" in window) {
          const utterance = new SpeechSynthesisUtterance(response.content);
          utterance.lang = voiceSettings.language;
          utterance.rate = voiceSettings.rate || 1;
          utterance.pitch = voiceSettings.pitch || 1;
          utterance.volume = voiceSettings.volume || 1;
          set({ isSpeaking: true });
          utterance.onend = () => set({ isSpeaking: false });
          speechSynthesis.speak(utterance);
        }
      }
      if (voiceSettings.autoAddToMessages && response.transcription) {
        const { addMessage } = get();
        addMessage({
          type: "text",
          role: "user",
          content: response.transcription,
          metadata: {
            source: "voice",
            timestamp: (/* @__PURE__ */ new Date()).toISOString()
          }
        });
      }
      const items = [];
      if (response.content) {
        items.push(response.content);
      }
      if (response.object) {
        if (Array.isArray(response.object)) {
          items.push(...response.object);
        } else {
          items.push(response.object);
        }
      }
      if (items.length > 0) {
        const { handleLLMResponse } = get();
        await handleLLMResponse(items);
      }
      get().setIsProcessing(false);
    } catch (error) {
      set({
        voiceError: error instanceof Error ? error.message : "Failed to process voice"
      });
      get().setIsProcessing(false);
    }
  },
  playAudioResponse: async (audioData) => {
    try {
      set({ isSpeaking: true });
      const audio = new Audio();
      if (typeof audioData === "string") {
        audio.src = audioData;
      } else {
        const blob = new Blob([audioData], { type: "audio/mpeg" });
        audio.src = URL.createObjectURL(blob);
      }
      audio.onended = () => {
        set({ isSpeaking: false });
        if (typeof audioData !== "string") {
          URL.revokeObjectURL(audio.src);
        }
      };
      await audio.play();
    } catch (error) {
      set({
        isSpeaking: false,
        voiceError: error instanceof Error ? error.message : "Failed to play audio"
      });
    }
  },
  setVoiceEndpoint: (endpoint) => {
    set((state) => ({
      voiceSettings: {
        ...state.voiceSettings,
        endpoint
      }
    }));
  },
  updateVoiceSettings: (settings) => {
    set((state) => ({
      voiceSettings: {
        ...state.voiceSettings,
        ...settings
      }
    }));
  },
  setVoiceError: (error) => {
    set({ voiceError: error });
  },
  resetVoiceState: () => {
    const { audioStream, audioContext, mediaRecorder } = get();
    if (mediaRecorder && mediaRecorder.state !== "inactive") {
      mediaRecorder.stop();
    }
    if (audioStream) {
      audioStream.getTracks().forEach((track) => track.stop());
    }
    if (audioContext && audioContext.state !== "closed") {
      audioContext.close();
    }
    set(initialVoiceState);
  }
});
// Annotate the CommonJS export names for ESM import in node:
0 && (module.exports = {
  createVoiceSlice
});
//# sourceMappingURL=index.js.map